{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOnJ3U+eXVxTR25Bw73H4Li"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jPnp4zJWUCZv"},"outputs":[],"source":["!pip install keras-cv==0.4.0 -q\n","!pip install -U tensorflow -q\n","!pip install webcolors==1.3\n","!pip install Random-Word"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"7cTgZQyTUEqu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textwrap import wrap\n","import os\n","import pathlib\n","\n","import keras_cv\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.experimental.numpy as tnp\n","from keras_cv.models.stable_diffusion.clip_tokenizer import SimpleTokenizer\n","from keras_cv.models.stable_diffusion.diffusion_model import DiffusionModel\n","from keras_cv.models.stable_diffusion.image_encoder import ImageEncoder\n","from keras_cv.models.stable_diffusion.noise_scheduler import NoiseScheduler\n","from keras_cv.models.stable_diffusion.text_encoder import TextEncoder\n","from tensorflow import keras\n","\n","import librosa\n","import librosa.display\n","import librosa.feature\n","from IPython.display import Audio\n","from IPython.display import Image as IImage\n","from PIL import Image\n","import random\n","from webcolors import rgb_to_name\n","import matplotlib.colors as mcolors\n","from moviepy.editor import *"],"metadata":{"id":"5f4l7ohpUMiB","executionInfo":{"status":"ok","timestamp":1682259239911,"user_tz":-60,"elapsed":5027,"user":{"displayName":"Javiera ZO","userId":"17073357613151539591"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Fine - Tuning a stable diffusion model\n","This step of the notebook is not necessary to run to generate outputs, because the fine-tuned weights have been stored in finetuned_stable_diffusion.h5. The dataset used for fine-tuning is the Describable Textures Dataset.\n"],"metadata":{"id":"liG1S-OhiIDy"}},{"cell_type":"code","source":["######## Fine-tuning stable diffusion code taken and modified from\n","######## https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/finetune_stable_diffusion.ipynb\n","######## Authored by Sayak Paul, Chansung Park\n","\n","######## FINE-TUNING THE MODEL ###################################\n","\n","# Load data\n","data_path = pathlib.Path('/content/gdrive/MyDrive/MyTextures')\n","data_frame = pd.read_csv(os.path.join(data_path, \"data.csv\"))\n","\n","data_frame[\"image_path\"] = data_frame[\"image_path\"].apply(\n","    lambda x: os.path.join(data_path, x)\n",")\n","data_frame.head()\n","\n","##################################################################\n","\n","# The padding token and maximum prompt length are specific to the text encoder.\n","PADDING_TOKEN = 49407\n","MAX_PROMPT_LENGTH = 77\n","\n","# Load the tokenizer.\n","tokenizer = SimpleTokenizer()\n","\n","#  Method to tokenize and pad the tokens.\n","def process_text(caption):\n","    tokens = tokenizer.encode(caption)\n","    tokens = tokens + [PADDING_TOKEN] * (MAX_PROMPT_LENGTH - len(tokens))\n","    return np.array(tokens)\n","\n","# Collate the tokenized captions into an array.\n","tokenized_texts = np.empty((len(data_frame), MAX_PROMPT_LENGTH))\n","\n","all_captions = list(data_frame[\"caption\"].values)\n","for i, caption in enumerate(all_captions):\n","    tokenized_texts[i] = process_text(caption)\n","\n","\n","####################################################################\n","\n","# Defining the model\n","RESOLUTION = 256\n","AUTO = tf.data.AUTOTUNE\n","POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\n","\n","augmenter = keras.Sequential(\n","    layers=[\n","        keras_cv.layers.CenterCrop(RESOLUTION, RESOLUTION),\n","        keras_cv.layers.RandomFlip(),\n","        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n","    ]\n",")\n","text_encoder = TextEncoder(MAX_PROMPT_LENGTH)\n","\n","\n","#####################################################################\n","\n","# Formatting the dataset\n","def process_image(image_path, tokenized_text):\n","    image = tf.io.read_file(image_path)\n","    image = tf.io.decode_png(image, 3)\n","    image = tf.image.resize(image, (RESOLUTION, RESOLUTION))\n","    return image, tokenized_text\n","\n","\n","def apply_augmentation(image_batch, token_batch):\n","    return augmenter(image_batch), token_batch\n","\n","\n","def run_text_encoder(image_batch, token_batch):\n","    return (\n","        image_batch,\n","        token_batch,\n","        text_encoder([token_batch, POS_IDS], training=False),\n","    )\n","\n","\n","def prepare_dict(image_batch, token_batch, encoded_text_batch):\n","    return {\n","        \"images\": image_batch,\n","        \"tokens\": token_batch,\n","        \"encoded_text\": encoded_text_batch,\n","    }\n","\n","\n","def prepare_dataset(image_paths, tokenized_texts, batch_size=1):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tokenized_texts))\n","    dataset = dataset.shuffle(batch_size * 10)\n","    dataset = dataset.map(process_image, num_parallel_calls=AUTO).batch(batch_size)\n","    dataset = dataset.map(apply_augmentation, num_parallel_calls=AUTO)\n","    dataset = dataset.map(run_text_encoder, num_parallel_calls=AUTO)\n","    dataset = dataset.map(prepare_dict, num_parallel_calls=AUTO)\n","    return dataset.prefetch(AUTO)\n","\n","# Prepare the dataset.\n","training_dataset = prepare_dataset(\n","    np.array(data_frame[\"image_path\"]), tokenized_texts, batch_size=4\n",")\n","\n","# Take a sample batch and investigate.\n","sample_batch = next(iter(training_dataset))\n","\n","for k in sample_batch:\n","    print(k, sample_batch[k].shape)\n","\n","plt.figure(figsize=(20, 10))\n","\n","for i in range(3):\n","    ax = plt.subplot(1, 4, i + 1)\n","    plt.imshow((sample_batch[\"images\"][i] + 1) / 2)\n","\n","    text = tokenizer.decode(sample_batch[\"tokens\"][i].numpy().squeeze())\n","    text = text.replace(\"<|startoftext|>\", \"\")\n","    text = text.replace(\"<|endoftext|>\", \"\")\n","    text = \"\\n\".join(wrap(text, 12))\n","    plt.title(text, fontsize=15)\n","\n","    plt.axis(\"off\")\n","\n","\n","##########################################################################\n","\n","#Setting up a trainer for the fine-tuning training\n","\n","class Trainer(tf.keras.Model):\n","    # Reference:\n","    # https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py\n","\n","    def __init__(\n","        self,\n","        diffusion_model,\n","        vae,\n","        noise_scheduler,\n","        use_mixed_precision=False,\n","        max_grad_norm=1.0,\n","        **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","\n","        self.diffusion_model = diffusion_model\n","        self.vae = vae\n","        self.noise_scheduler = noise_scheduler\n","        self.max_grad_norm = max_grad_norm\n","\n","        self.use_mixed_precision = use_mixed_precision\n","        self.vae.trainable = False\n","\n","    def train_step(self, inputs):\n","        images = inputs[\"images\"]\n","        encoded_text = inputs[\"encoded_text\"]\n","        batch_size = tf.shape(images)[0]\n","\n","        with tf.GradientTape() as tape:\n","            # Project image into the latent space and sample from it.\n","            latents = self.sample_from_encoder_outputs(self.vae(images, training=False))\n","            # Know more about the magic number here:\n","            # https://keras.io/examples/generative/fine_tune_via_textual_inversion/\n","            latents = latents * 0.18215\n","\n","            # Sample noise that we'll add to the latents.\n","            noise = tf.random.normal(tf.shape(latents))\n","\n","            # Sample a random timestep for each image.\n","            timesteps = tnp.random.randint(\n","                0, self.noise_scheduler.train_timesteps, (batch_size,)\n","            )\n","\n","            # Add noise to the latents according to the noise magnitude at each timestep\n","            # (this is the forward diffusion process).\n","            noisy_latents = self.noise_scheduler.add_noise(\n","                tf.cast(latents, noise.dtype), noise, timesteps\n","            )\n","\n","            # Get the target for loss depending on the prediction type\n","            # just the sampled noise for now.\n","            target = noise  # noise_schedule.predict_epsilon == True\n","\n","            # Predict the noise residual and compute loss.\n","            timestep_embedding = tf.map_fn(\n","                lambda t: self.get_timestep_embedding(t), timesteps, dtype=tf.float32\n","            )\n","            timestep_embedding = tf.squeeze(timestep_embedding, 1)\n","            model_pred = self.diffusion_model(\n","                [noisy_latents, timestep_embedding, encoded_text], training=True\n","            )\n","            loss = self.compiled_loss(target, model_pred)\n","            if self.use_mixed_precision:\n","                loss = self.optimizer.get_scaled_loss(loss)\n","\n","        # Update parameters of the diffusion model.\n","        trainable_vars = self.diffusion_model.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        if self.use_mixed_precision:\n","            gradients = self.optimizer.get_unscaled_gradients(gradients)\n","        gradients = [tf.clip_by_norm(g, self.max_grad_norm) for g in gradients]\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def get_timestep_embedding(self, timestep, dim=320, max_period=10000):\n","        half = dim // 2\n","        log_max_preiod = tf.math.log(tf.cast(max_period, tf.float32))\n","        freqs = tf.math.exp(\n","            -log_max_preiod * tf.range(0, half, dtype=tf.float32) / half\n","        )\n","        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n","        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n","        embedding = tf.reshape(embedding, [1, -1])\n","        return embedding\n","\n","    def sample_from_encoder_outputs(self, outputs):\n","        mean, logvar = tf.split(outputs, 2, axis=-1)\n","        logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n","        std = tf.exp(0.5 * logvar)\n","        sample = tf.random.normal(tf.shape(mean), dtype=mean.dtype)\n","        return mean + std * sample\n","\n","    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n","        # Overriding this method will allow us to use the `ModelCheckpoint`\n","        # callback directly with this trainer class. In this case, it will\n","        # only checkpoint the `diffusion_model` since that's what we're training\n","        # during fine-tuning.\n","        self.diffusion_model.save_weights(\n","            filepath=filepath,\n","            overwrite=overwrite,\n","            save_format=save_format,\n","            options=options,\n","        )\n","\n","\n","##############################################################################\n","\n","# Start training process\n","\n","# Enable mixed-precision training if the underlying GPU has tensor cores.\n","USE_MP = True\n","if USE_MP:\n","    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n","\n","image_encoder = ImageEncoder(RESOLUTION, RESOLUTION)\n","diffusion_ft_trainer = Trainer(\n","    diffusion_model=DiffusionModel(RESOLUTION, RESOLUTION, MAX_PROMPT_LENGTH),\n","    # Remove the top layer from the encoder, which cuts off the variance and only\n","    # returns the mean.\n","    vae=tf.keras.Model(\n","        image_encoder.input,\n","        image_encoder.layers[-2].output,\n","    ),\n","    noise_scheduler=NoiseScheduler(),\n","    use_mixed_precision=USE_MP,\n",")\n","\n","# These hyperparameters come from this tutorial by Hugging Face:\n","# https://huggingface.co/docs/diffusers/training/text2image\n","lr = 1e-5\n","beta_1, beta_2 = 0.9, 0.999\n","weight_decay = (1e-2,)\n","epsilon = 1e-08\n","\n","optimizer = tf.keras.optimizers.experimental.AdamW(\n","    learning_rate=lr,\n","    weight_decay=weight_decay,\n","    beta_1=beta_1,\n","    beta_2=beta_2,\n","    epsilon=epsilon,\n",")\n","diffusion_ft_trainer.compile(optimizer=optimizer, loss=\"mse\")\n","\n","# Actually train. 70 epochs chosen because the source stated that was the number \n","#of iterations for which they got the best resuls\n","\n","epochs = 70\n","ckpt_path = \"/content/gdrive/MyDrive/finetuned_stable_diffusion.h5\"\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(\n","    ckpt_path,\n","    save_weights_only=True,\n","    monitor=\"loss\",\n","    mode=\"min\",\n",")\n","diffusion_ft_trainer.fit(training_dataset, epochs=epochs, callbacks=[ckpt_callback])"],"metadata":{"id":"YVXNyaVDUnqJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generating outputs\n","From here we can load the fine-tuned weights and use these to generate images inspired by an input audio. We then perform interpolations between prompts parametrised by musical characteristics to walk through the latent space and output short videos with the audio."],"metadata":{"id":"xplTJJf5kghc"}},{"cell_type":"code","source":["######## Loading fine-tuned weights\n","weights_path = '/content/gdrive/MyDrive/finetuned_stable_diffusion.h5'\n","\n","img_height = img_width = 512\n","textures_model = keras_cv.models.StableDiffusion(\n","    img_width=img_width, img_height=img_height\n",")\n","# We just reload the weights of the fine-tuned diffusion model.\n","textures_model.diffusion_model.load_weights(weights_path)\n","\n","###########################################################################\n","\n","######## Loading audio file\n","\n","audioname = input('Enter a filepath for a .wav audio file: ')\n","\n","if 'wav' not in audioname:\n","  print('Not a .wav file, please try again.')\n","\n","y, sr = librosa.load(audioname)\n","\n","clip_length_seconds = 15\n","if len(y) > clip_length_seconds * sr:\n","  y = y[:clip_length_seconds*sr]\n","\n","\n","######## Processing audio file\n","\n","X = np.abs(librosa.stft(y)) # get STFT representatino\n","\n","SC = librosa.feature.spectral_centroid(y=y, sr=sr) # get spectral centroids over time\n","\n","onset_env = librosa.onset.onset_strength(y=y, sr=sr) # get spectral flux over time\n","\n","zcr = librosa.feature.zero_crossing_rate(y)[0] # get zero crossing rate over time\n","\n","XT = X.T\n","volume = []\n","prompts = []\n","\n","# Define some prompts vaguely inspired by timbral dimensions\n","high_centroid = ['bright', 'shiny', 'up', 'light', 'airy']\n","low_centroid = ['dark', 'dull', 'unsettling', 'down', 'deep']\n","high_flux = ['intense', 'swirl', 'thick', 'colourful', 'irregular']\n","low_flux = ['undulating', 'regular', 'round', 'sphere', 'bland']\n","high_zcr = ['rough', 'harsh', 'crackly', 'bumpy', 'intricate', 'pointed']\n","low_zcr = ['flat', 'smooth', 'soft', 'square', 'satin', 'linear']\n","\n","######## Mappings ########\n","\n","def map_SC_to_prob(SC):\n","    # Define the height range for the color mapping\n","    SC_min = 0\n","    SC_max = 2000\n","    \n","    # Define the probability range for the color mapping\n","    prob_min = 0.1\n","    prob_max = 0.9\n","    \n","    # Map the height value to the probability range using a linear mapping function\n","    prob = (SC - SC_min) / (SC_max - SC_min) * (prob_max - prob_min) + prob_min\n","    \n","    return prob\n","\n","# Making a dictionary for colour prompts\n","\n","def get_rgb(name):\n","    rgb_tuple = mcolors.to_rgb(name)\n","    return rgb_tuple\n","\n","colours_dict = mcolors.CSS4_COLORS\n","sorted_colours = sorted(colours_dict, key=get_rgb)\n","\n","\n","prob = map_SC_to_prob(SC[0][0]) \n","\n","######## Prompt creation ########\n","print('Creating prompts...')\n","prompt_step = 40\n","for i in range(0,len(SC[0]), prompt_step):\n","\n","    # Spectral centroid prompt\n","    prob = map_SC_to_prob(SC[0][i])\n","    rand_num = random.random()\n","\n","    if prob >= rand_num:\n","        rand_ind = random.randrange(len(high_centroid)-1)\n","        hue = high_centroid[rand_ind]\n","    else:\n","        rand_ind = random.randrange(len(low_centroid)-1)\n","        hue = low_centroid[rand_ind]\n","\n","    # Spectral flux prompt\n","    if onset_env[i] > 2:\n","        rand_ind = random.randrange(len(high_flux)-1)\n","        flux = high_flux[rand_ind]\n","    else:\n","        rand_ind = random.randrange(len(low_flux)-1)\n","        flux = low_flux[rand_ind]\n","\n","    # Zero crossing rate prompt\n","    if zcr[i] > 0.05:\n","        rand_ind = random.randrange(len(high_zcr)-1)\n","        texture = high_zcr[rand_ind]\n","    else:\n","        rand_ind = random.randrange(len(low_zcr)-1)\n","        texture = low_zcr[rand_ind]\n","\n","    # Colours prompt\n","    volume = np.mean(XT[i])\n","    map_colours = (volume-min(XT[i]))/(max(XT[i])-min(XT[i])+0.01) * (147*2)\n","    index = int(map_colours)\n","    colour = sorted_colours[index]\n","\n","    string = hue + ' ' + flux + ' ' + texture + ' ' + colour\n","    prompts.append(string)\n","\n","#######################################################################\n","\n","# Walk through the latent diffusion space:\n","# https://keras.io/examples/generative/random_walks_with_stable_diffusion/\n","# Authored by Ian Stenbit, fchollet, lukewood\n","\n","# Helper function from https://keras.io/examples/generative/random_walks_with_stable_diffusion/\n","# Creates gifs\n","\n","print('Setting up image generation...')\n","def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):\n","    if rubber_band:\n","        images += images[2:-1][::-1]\n","    images[0].save(\n","        filename,\n","        save_all=True,\n","        append_images=images[1:],\n","        duration=1000 // frames_per_second,\n","        loop=0,\n","    )\n","\n","# Encode\n","encoding = []\n","for prompt in prompts:\n","  encoding.append(tf.squeeze(textures_model.encode_text(prompt)))\n","\n","# Set parameters for generation\n","interpolation_steps = 6\n","batch_size = 3\n","batches = (interpolation_steps) // batch_size\n","seed = 12345\n","noise = tf.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n","images = []\n","\n","\n","# Interpolate between pairs of prompts for the length\n","# of the prompts array\n","print('Generating gifs...')\n","for i in range(len(encoding)-1):\n","  interpolations = tf.linspace(encoding[i], encoding[i+1], interpolation_steps)\n","  batched_encodings = tf.split(interpolations, batches)\n","\n","  for batch in range(batches):\n","    images += [\n","        Image.fromarray(img)\n","        for img in textures_model.generate_image(\n","            batched_encodings[batch],\n","            batch_size=batch_size,\n","            num_steps=25,\n","            diffusion_noise=noise,\n","        )\n","    ]\n","if 'wav' in audioname:\n","  gif_name = audioname.strip('wav') + 'gif'\n","elif 'mp3' in audioname:\n","  gif_name = audioname.strip('mp3') + 'gif'\n","export_as_gif(gif_name, images, rubber_band=True)\n","  \n","######## Combine video with audio\n","\n","print('Creating video...')\n","\n","video_clip = VideoFileClip(gif_name)\n","audio_clip = AudioFileClip(audioname)\n","audio_clip = audio_clip.subclip(0, clip_length_seconds)\n","final_clip = video_clip.set_audio(audio_clip)\n","audioname = audioname.strip('mp3')\n","output_filename = audioname.strip('wav') + 'mp4'\n","final_clip.write_videofile(output_filename)"],"metadata":{"id":"Rn6iUgD6XWJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JqIDA5ZIw7tn"},"execution_count":null,"outputs":[]}]}